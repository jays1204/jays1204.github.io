---
layout: post
title: " ML study Week 01 "
date: 2018-03-11 16:16:00
categories: ML MachineLearning Regression
---

# Before  
이 포스트는 다음의 강의를 듣고 개인적으로 공부하면서 요약한 부분이다. 
https://hunkim.github.io/ml/

# Machine Learning Study Week 01  

## 머신러닝의 개념과 용어 비디오  (TensorFlow의 기본 Lab 비디오 ) 강의 슬라이드  Lab 슬라이드  

  - Supervised Learning  
    - 이미 분류된 (레이블이 정의된) 데이터셋이 주어져 있다. 이 데이터 셋을 갖고 학습을 한다.
  - Unsupervised learning
    - clustering한다. 즉 아직 레이블이 없는 데이터 셋에 대해 레이블을 만든다.

  - Training Data set
    - ML모델을 만들어내기 위해 사전에 준비된 데이터들이다.   
      Supervised Learning인 경우에는 X값과 더불어 이미 레이블이 정해져있는 Y 값 까지 존재한다.   
      이런 셋을 갖고 학습을 하게 되고  이를 통해 모델이 생겨난다.  
      이렇게 만들어진 모델을 통해 새로운 X에 대해 레이블을 알 수 있다.  
      이 트레이닝 데이터 셋이 있어야 ML 모델을 만들 수 있다.  

  -  Types of Supervised learning  
    - regression : 범위에 대해 (0~100) 알아보는 것을 regression 이라고 한다. 예를 들면 공부 시간에 의거해 시험 성적 알아보기  
    - binary classfication : Boolean 결과값에 대해 알아보는 Boolean binary classification이다. 예를 들면  공부 시간에 의거해 패스했냐 패스 못했냐를 알아보기  
    - mutli-label classification : 결과 값으로 몇 가지의 주어진 레이블이 있고 이에 대해 알아보는 것이다. 예를 들면  공부 시간에 의거해 성적 등급 알아보기.   
            즉 몇가지 레이블 -Grade(A, B, C, D, F)-를 알아보기 는 mutli-label classfication이라고 한다.


## Linear Regression 의 개념 비디오  (TensorFlow 로 구현 Lab 비디오 ) 강의 슬라이드  Lab 슬라이드   

- Linear Regression  
  - 범위를 예측하는 것은 Supervised 중에서도 regression이라고 부른다.  
  - input인 x는 예측을 하기위한 기본적인 자료 또는 featuer라고 말한다.  

  - Linear Hypothesis  
    - 모델이 Linear할거라고 가정하는 것이 Linear Regression 이다.   
    - 예를 들면 공부 시간에 비례 해 성적이 높아진다, 훈련에 비래해 달리기 속도가 높아진다는 등  
    - 어떤 데이터가 있다면 이 데이터셋에 잘 들어맞는 Linear한 선을 찾는 다는 것을 Linear Regression 학습이다.  

  - 가정에 의해 얻어질 값을 나타내는 수식  H(x) = Wx + b 의 형태이다.    
  - Hypothesis가 이런 일차 방정식이 될거라고 가설을 세우는 것이 Linear Regression의 첫번째 과정이다.  
    -  이렇게 나타난 여러가지 선들중 어떤게 가장 잘 맞는 모델일지 찾아야 한다.  

  - 어떤 Hypothesis가 더 좋은가?  
    - 실제 데이터와 가설의 선- H(x) -과 거리가 가까울 수록 좋은 Hypothesis 다.  
    - 이 거리를 측정하는 것을 Cost Function 또는 loss function이라고 한다.  
  - Cost Function : H(x) - y  
    - 우리가 만든 Hypothesis 모델의 값인 H(x)와 데이터 값인 y의 차이를 구한다.  
    - 보통 이 차이에 대해 제곱을 한다.   
      - 이러면 좋은 점이 차이의 값이 일정하게 양수로 표현해 주고 제곱이 있으므로 차이가 클 수록 페널티가 커지는 효과가 있다.   
  - 이 차의 제곱에 대해 i=0...m 까지를 적용하여 sum한 후 m 으로 나누면 cost가 나온다.
  - m 은 데이터 개수를 말한다.  

  - Linear Regression의 숙제는 가장 작은 Cost를 만드는  W, b를 구하는 것이다.  
    - Cost Function :  Cost(W, b) = 1/m * sum((H(x) - y)^2)
  - 위의 Cost Function을 최소화하기 위한 W, b를 구해야 한다.  


## Linear Regression cost함수 최소화 비디오  (TensorFlow 로 구현 Lab 비디오 ) 강의 슬라이드  Lab 슬라이드   
  - Minimize Cost  
    - H(x) = Wx + b의  Hypothesis가 있고 실제 데이터와 얼마가 차이가 있는지 나타내는 Cost(W, b) 가 있다.  
    - 이 Cost를 minimize하는 W, b를 구하자.
  
  - Simplified Hypothesis   
    - H(x) = Wx  
      - 계산을 편하게 하기 위해 임의로 b는 우선 뺴자  
    - Cost(W)   
      - W의 값에 따라 나온 Cost(W)의 그래프를 그려본다.   
      - 그래프를 눈으로 보면 가장 작은게 보이는데 수식으로는 어떻게 찾아야 할까?

    - Gradient descent Algorithm  
    - 최소의 Cost(W)값을 찾기 위위해 사용하는 것이 Gradient descent Alg 이다.  
      - 주어진 cost funciton을 minimize하는데 많이 사용한다.  
    - How it works?  
      - Cost(W)를 표현한 그래프의 어떤 점에서도 시작 가능하다. 
      - 이제 Cost(W, b)의 값을 줄이기 위해 W, b를 조금 바꿔본다. 
        - 이제 경사도를 계산해서 더 높은 경사도를 찾아보고 이를 반복하다보면 가장 낮은 경사도를 찾게 될 것이다.
      - 바꾸다보면 더이상 값이 변하지 않게 되는 순간이 오고 그 때의 W가 바로 최소의 Cost(W) 를 말하는 지점이다.  

    - 경사도를 찾기 위해 미분이 필요하다.  
      - 미분을 쉽게 적용하기 위해 cost(W)의 수식에서 1/m 대신에 1/2m 으로 바꾼다.
      - Alpha 라고 하는 값은 learning rate로 우선 상수로 본다. 이제 cost(W)의 (편)미분한 값, 즉 기울기의 값을 Alpha에 곱한다. 
        - Alpha는 얼마나 움직일지. 보폭의 크기 정도라 생각하자.
    - 즉 Gradient descent alg 식의 결과는 우선 기존의 W보다 더 최소의 값에 가까운걸 찾아내면서 좌우로 움직일 수 있다.  
    - 즉 편미분한 값인 기울기를 W에서 뺴준다는건 기울기가 양수면 W보다 좌측으로 가라, 음수면 W보다 우측으로 가라 라는 의미이다.  


    - Convex function  
    - 출발점에 따라 최저 Cost가 달라져 W, b 도 달라지는 형태가 잇을수 있는데 이런게 아니라 어느점에서 시작하건간에 최소 지점으로 도착하는 function이다.   
    이 경우에는 Gradient descent alg가 항상 답을 찾는 것을 보장한다.  
    즉 cost function의 모형이 Convex function 이 되는 것을 확인해야 한다.


## 여러개의 입력(feature)의 Linear Regression 비디오  (TensorFlow 로 구현 Lab1 비디오 ) (파일 데이타 로딩 Lab2   

- Multi variable Linear regression  
    - 여지껏 input은 하나였는데 input이 여러개이면 어떻게 하나?
    - Hypothesis는 다음과 같이 된다.   
      - H(x1, x2, x3) = W1x1 + W2x2 + W3x3 + b
      - x들에 대해 같은 linear로 적용(곱)해주면 된다.  
    - multi를 matrix로 표현하여 Matrix doc production 이용한다.   
    - ```code
      (x1, x2, x3) * (w1
                      w2        = (x1w1 + x2w2 + x3w3)
                      w3)

      ```  
    - H(X) = XW  
      - 대문자로 쓴건 matrix 라는 의미이다. 
      - matrix 쓸때는 보통 기존과 다르게 x를 앞에 쓴다. (기존은 w가 앞에옴)

    - instance: multi value x1, x2, x3, ...를 묶어서 하나를 instance라고 함  
    - matrix이용한 Hypothesis    
      - 보통 instance인 X의 셋은 주어진다. H(X)도 주어진다. W는 모른다.
      - TensorFlow는 x의 rows가 미정일때 None 이라고 한다.





- References  
  - https://www.youtube.com/watch?v=qPMeuL2LIqY  
  - https://hunkim.github.io/ml/lec1.pdf  
  - https://www.youtube.com/watch?v=Hax03rCn3UI  
  - https://www.youtube.com/watch?v=TxIVr-nk1so  
  - http://aikorea.org/cs231n/optimization-1/  
  - http://studioplug.tistory.com/252  
  - http://daeson.tistory.com/167  
  - https://www.youtube.com/watch?v=kPxpJY6fRkY&feature=youtu.be  
